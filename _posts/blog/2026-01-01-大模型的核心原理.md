---
layout: post
title: "大模型的核心原理"
date: 2026-01-01
last_modified_at: 2026-01-01
categories: [技术纵横]
tags: [大模型]
cover: https://qiniu.zhuyucun.cn/uploads/1764853276621_kn1jl7.jpg
excerpt: >
  从神经网络到transformer
---

### 背景与问题
神经网络：全称为人工神经网络（ANN），是模仿人类大脑神经网络系统启发而设计的计算模型。它由大量的相互连接的节点，或者称之为神经元组成。

深度学习：特指深层神经网络模型的机器学习。

Transformer 本质上是一个深度神经网络模型，依赖于前馈神经网络FNN（Feedforward Neural Networks）和反向传播算法来训练模型。transformer架构也是属于深度学习/神经网络的一种

1、卷积神经网络（CNN）  
2、循环神经网络（RNN）  
3、长短时记忆网络（LSTM）  

### transformer的优势：  
1、并行计算  
2、捕捉长距离依赖  
3、可扩展性  
4、灵活性和效果  

### 模型训练过程  
- 监督学习
- 无监督学习
- 自监督学习
- 强化学习  

预训练阶段，模型在大规模无标注语料上学习通用语言知识；随后通过监督微调（SFT），用行业高质量标注数据让模型掌握垂直领域知识；最后借助基于人类反馈的强化学习（RLHF）训练奖励模型，使输出对齐人类偏好，实现持续优化。

### transformer架构
本次主要是基于《Attention Is All You Need》这篇论文来介绍transformer架构。
架构图如下：
![transformer架构图](https://qiniu.zhuyucun.cn/uploads/1767274855060_a4oxf2.png)

第一步： 输入的预处理  
1、分词： 将输入的文本序列切分成离散的标记（tokens），例如单词、子词或字符。  
2、词嵌入： 将每个标记映射到一个固定维度的向量表示，捕捉标记的语义信息。  
3、位置编码： 由于Transformer模型不考虑序列中的位置信息，需要添加位置编码来引入序列中每个标记的位置。  
4、结合词嵌入和位置编码： 将词嵌入和位置编码相加，得到每个标记的最终表示。  

第二步：编码器处理  
1、多头自注意力机制： 编码器使用多头自注意力机制来捕捉输入序列中不同位置之间的依赖关系。每个头都有不同的注意力权重，能够关注到不同的特征。  
2、前馈神经网络： 每个编码器层都有一个前馈神经网络，用于对自注意力机制的输出进行非线性变换。  
3、残差连接+层归一化： 每个解码器层都有一个残差连接，将输入与前馈神经网络的输出相加。然后，对残差连接的输出进行层归一化。  

第三步：解码器处理  
1、多头自注意力机制： 解码器也使用多头自注意力机制，但是它还考虑了编码器的输出。  
2、编码器-解码器注意力机制： 解码器使用编码器-解码器注意力机制，将编码器的输出与当前位置的输入进行结合，捕捉到上下文信息。  
3、前馈神经网络： 每个解码器层都有一个前馈神经网络，用于对结合后的表示进行非线性变换。  
4、残差连接+层归一化

第四步：输出  
1、线性层： 解码器的输出通过一个线性层映射到词汇表的维度，得到每个位置上每个标记的概率分布。  
2、softmax函数： 对线性层的输出应用softmax函数，将其归一化到概率分布。

### 参考
- [Attention Is All You Need](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)   

- [LLM Visualization](https://bbycroft.net/llm)

- [TRANSFORMER EXPLAINER](https://poloclub.github.io/transformer-explainer/)

